# Agent A6 ‚Äì SEO (Search Engine Optimization)

---

## 1. √úbersicht / Rolle im System

Der **SEO Agent (A6)** analysiert die Inhalte einer Website (aus A2: CrawlResult und A3: ContentExtract) im Hinblick auf **Suchmaschinenoptimierung**.
Ziel: systematische Pr√ºfung und Erkennung von SEO-Schwachstellen und -Potenzialen. A6 liefert konkrete Findings und Kennzahlen, die sp√§ter in den **Renewal Plan (A10)** und das **Angebotsdokument (A15)** einflie√üen.

W√§hrend A3 f√ºr Text-Extraktion zust√§ndig ist, setzt A6 darauf auf und √ºberpr√ºft Struktur, Metadaten und interne Verlinkung.

---

## 2. Eingaben / Ausgaben (Schemas)

### Eingabe

* **CrawlResult.json**
* **ContentExtract.json**

Beispiel:

```json
{
  "pages": [
    {
      "url": "https://www.example.com/",
      "title": "Beispiel",
      "headings": ["Willkommen"],
      "text": "Unsere Leistungen‚Ä¶",
      "html": "<html><head><title>Beispiel</title><meta name='description' content=''></head></html>"
    }
  ]
}
```

### Ausgabe

* **SEOReport.json**

```json
{
  "issues": [
    {
      "id": "missing-meta-description",
      "where": ["/", "/leistungen"]
    },
    {
      "id": "no-canonical",
      "where": ["/"]
    },
    {
      "id": "duplicate-title",
      "where": ["/leistungen", "/angebot"]
    }
  ],
  "sitemap": {
    "found": true,
    "urls": 120
  },
  "linking": {
    "internalLinks": 85,
    "brokenLinks": ["https://old.example.com/page-x"]
  },
  "schemaOrg": {
    "detected": ["LocalBusiness", "BreadcrumbList"],
    "missing": ["FAQPage"]
  }
}
```

---

## 3. Interne Abh√§ngigkeiten

### Libraries / Technologien

* **Parser**: BeautifulSoup/lxml zum Analysieren von `<title>`, `<meta>`, `<link rel="canonical">`
* **Sitemap-Parser**: `sitemap-parser` oder eigene XML-Auswertung
* **Link-Checker**: HTTP-HEAD-Anfragen (Fetch MCP) zur Validierung interner/externer Links
* **Schema.org Detection**: JSON-LD-Bl√∂cke im `<script type="application/ld+json">` parsen
* **Duplicate Detection**: Hashing von Titles, Meta-Descriptions

### Algorithmen

* **Meta-Check**: Pr√ºfen, ob jede Seite Title/Description hat, ob L√§nge passt
* **Canonical-Check**: `<link rel="canonical">` vorhanden?
* **Sitemap-Existenz**: /sitemap.xml, robots.txt ‚Üí Eintr√§ge z√§hlen
* **Internal Linking**: Linkgraph erstellen, Broken Links per Fetch pr√ºfen
* **Schema.org**: vorhandene Typen extrahieren, fehlende empfehlen

---

## 4. Externe Abh√§ngigkeiten

* **Filesystem MCP**: Speichern der SEOReports
* **Fetch MCP**: Abfragen von /robots.txt, /sitemap.xml, Broken Links
* **Playwright MCP (optional)**: DOM nach dynamisch erzeugten Metadaten durchsuchen
* **Memory MCP**: Delta-Analyse zwischen Scans (‚ÄûMeta-Description erg√§nzt am 2025-09-01‚Äú)

---

## 5. Ablauf / Workflow intern

1. **Initialization**

   * CrawlResult & ContentExtract laden
   * Liste aller URLs erstellen

2. **Meta-Checks**

   * `<title>` pr√ºfen: L√§nge 30‚Äì65 Zeichen, keine Duplicates
   * `<meta name=description>` pr√ºfen: L√§nge 50‚Äì160 Zeichen, keine Leere

3. **Canonical & Robots**

   * Pr√ºfen auf `<link rel=canonical>`
   * robots.txt analysieren (Disallow-Regeln, Sitemap-Hinweis)

4. **Sitemap Parsing**

   * /sitemap.xml laden
   * Anzahl URLs z√§hlen
   * Vergleichen mit gecrawlten Seiten

5. **Link-Analyse**

   * Alle internen Links sammeln
   * Broken Links via Fetch pr√ºfen
   * Link-Graph-Analyse (Â≠§Á´ã Pages identifizieren)

6. **Schema.org Analyse**

   * JSON-LD Bl√∂cke auslesen
   * Typen (LocalBusiness, FAQPage, BreadcrumbList) erkennen
   * Fehlende Typen als Empfehlung

7. **Scoring**

   * Score 0‚Äì100 pro Kategorie (Meta, Canonical, Links, Schema.org)
   * Aggregiert in Gesamt-SEO-Score

8. **Output**

   * SEOReport.json schreiben

---

## 6. Quality Gates & Non-Functional Requirements

* **Coverage**: Alle Seiten aus CrawlResult pr√ºfen
* **Performance**: ‚â§ 2 Sekunden pro Seite (ohne externe Requests)
* **Genauigkeit**: Broken Links nur mit HEAD/GET validieren, keine False Positives
* **Erweiterbarkeit**: Neue SEO-Regeln leicht hinzuf√ºgbar
* **Output**: Jede Issue mit ID, Beschreibung, betroffenen URLs

---

## 7. Skalierung, Deployment, Betrieb

* **Deployment**: Containerisierbarer Service (Python oder Java)
* **Scaling**: Parallel pro Domain oder pro 100 URLs
* **Persistence**: Memory MCP f√ºr SEO-Historie (z. B. ‚ÄûMeta-Descriptions hinzugef√ºgt‚Äú)
* **Observability**: Logs mit Anzahl Issues, Score pro Kategorie, Broken Link Count

---

## 8. Erweiterungspotenzial & offene Fragen

* **Mobile SEO**: Soll A6 auch Mobile Rendering pr√ºfen (Viewport, Mobile-Friendliness)?
* **PageSpeed**: Soll Performance (CLS, LCP, FID) Teil von SEO-Agent werden oder bei A7 Security/Performance?
* **Backlinks**: Externe Links analysieren (Ranking-Signal) ‚Üí evtl. eigenes Modul?
* **Keyword Density**: Soll A6 Content-Analyse mit Keywords durchf√ºhren oder A11 Rewriter?

---

üìÑ **Fazit**:
A6 ist der **Suchmaschinen-Optimierer** im Agentensystem. Er macht die Website f√ºr Google & Co. sichtbar, sorgt f√ºr Meta-/Struktur-Vollst√§ndigkeit und liefert wertvolle Eingaben f√ºr Rewrites (A11) und Angebote (A15).
