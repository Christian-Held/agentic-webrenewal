# Agent A2 ‚Äì Crawler

---

## 1. √úbersicht / Rolle im System

Der **Crawler (A2)** ist das zentrale Modul f√ºr die Datenerhebung im Renewal-System. Er √ºbernimmt den in **A1 (Scope/Seed)** definierten **ScopePlan** und f√ºhrt darauf basierend einen vollst√§ndigen Crawl durch.

Seine Hauptaufgabe ist es, **alle relevanten Seiten, Assets und Metadaten** einer Website zuverl√§ssig zu erfassen, sodass nachgelagerte Agenten (A3‚ÄìA9) auf valide, konsistente Daten zugreifen k√∂nnen.

Wesentliche Eigenschaften:

* Unterst√ºtzung von **statischen und dynamischen Inhalten** (JS-Rendering via Playwright)
* Sammlung von **HTML, Headers, Assets (Bilder, CSS, JS), Links**
* Erkennung von **Fehlerseiten (404/500)**
* Speicherung aller Rohdaten in einem strukturierten JSON-Format (**CrawlResult**)

---

## 2. Eingaben / Ausgaben (Schemas)

### Eingabe

* **ScopePlan.json** (von A1 erzeugt)

  ```json
  {
    "root": "https://www.example.com",
    "maxPages": 200,
    "include": ["/", "/leistungen", "/kontakt"],
    "exclude": ["/admin", "/login"],
    "respectRobots": true,
    "locales": ["de-DE"]
  }
  ```

### Ausgabe

* **CrawlResult.json**

  ```json
  {
    "pages": [
      {
        "url": "https://www.example.com/",
        "status": 200,
        "headers": {
          "content-type": "text/html; charset=utf-8",
          "content-length": "5234"
        },
        "html": "<!doctype html><html>‚Ä¶</html>",
        "links": ["/leistungen","/kontakt"],
        "images": [
          {"src":"/img/hero.jpg","alt":"","bytes":820000,"format":"jpeg"}
        ],
        "scripts": [
          {"src":"/js/app.min.js","integrity":null}
        ],
        "rendered": true,
        "timestamp": "2025-09-26T11:00:00Z"
      }
    ]
  }
  ```

---

## 3. Interne Abh√§ngigkeiten

### Libraries / Technologien

* **HTTP/Fetch**: f√ºr schnelle HEAD/GET-Anfragen (z. B. kleine Assets, Robots.txt-Rechecks)
* **Playwright**: zum Rendern dynamischer Inhalte (SPAs, Lazy-Loading, JS-generierte DOMs)
* **Queue-System**: z. B. asyncio-Queue oder Kafka (bei Java: Spring Kafka Listener) ‚Üí f√ºr paralleles Crawling
* **Parser**: BeautifulSoup/lxml f√ºr Linkextraktion
* **Rate-Limiter**: Verhindert √úberlastung der Zielseite
* **Checksum/Hasher**: zur Erkennung von Duplicate Content

### Algorithmen

* **Frontier Management**: BFS/DFS √ºber erlaubte Links, Respektierung von `maxPages`
* **Duplicate Detection**: Canonical-URLs + Checksums verhindern redundantes Crawlen
* **Render-Decision**: Heuristik entscheidet, ob Playwright-Rendering n√∂tig ist (z. B. bei `<script src>` ohne sichtbaren Content)
* **Retry-Strategy**: Exponentielles Backoff bei 429/503

---

## 4. Externe Abh√§ngigkeiten

* **Playwright MCP**: steuert Headless Chromium/Firefox/Webkit
* **Fetch MCP**: f√ºr schnelle HTTP-Requests
* **Filesystem MCP**: Speichern von CrawlResult.json und archivierten HTML-Seiten
* **Input vom Nutzer** (optional): Crawl-Strategie (z. B. nur bestimmte Unterverzeichnisse)

---

## 5. Ablauf / Workflow intern

1. **Initialization**

   * ScopePlan laden
   * Frontier mit Root-URL initialisieren

2. **Fetch Cycle**

   * URL aus Frontier entnehmen
   * Vorpr√ºfung: Exclude-Regeln anwenden
   * HTTP-Request via Fetch MCP ‚Üí Header + Body abrufen
   * Falls Content-Type `text/html` ‚Üí Render-Decision

3. **Rendering** (optional)

   * Playwright √∂ffnen
   * Seite laden, warten auf `networkidle`
   * DOM extrahieren (inkl. Shadow DOM)
   * Screenshots (optional f√ºr Preview-Agent)

4. **Parsing**

   * Links extrahieren und normalisieren
   * Bilder + Attribute (src, alt, Gr√∂√üe)
   * Scripts (src, inline code hash)
   * Stylesheets (src, inline length)

5. **Storage**

   * Ergebnisse als Page-Objekt in CrawlResult aufnehmen
   * Roh-HTML im Filesystem ablegen (`sandbox/raw/<hash>.html`)

6. **Iteration**

   * Neue Links in Frontier aufnehmen, solange `maxPages` nicht √ºberschritten
   * Schleife bis Frontier leer oder Limit erreicht

7. **Finalisierung**

   * CrawlResult.json persistieren
   * Metadaten speichern (Laufzeit, Anzahl gefundener Seiten)

---

## 6. Quality Gates & Non-Functional Requirements

* **Abdeckung**: ‚â• 95 % der in ScopePlan enthaltenen Seiten m√ºssen erfasst werden
* **Respekt von Regeln**: Robots.txt, maxPages und Exclude-Regeln immer beachten
* **Performance**: Crawl von 200 Seiten ‚â§ 5 Minuten (bei 1MB/Page, mittlerer Latenz)
* **Schonung**: Rate-Limits respektieren (z. B. max. 5 parallele Requests, 500ms Delay)
* **Zuverl√§ssigkeit**: Duplicate Detection, keine Endlosschleifen
* **Resilienz**: Fehlerhafte Seiten (404/500) werden markiert, aber blockieren nicht den Crawl
* **Transparenz**: Logs mit Crawled/Skipped/Errored URLs

---

## 7. Skalierung, Deployment, Betrieb

* **Deployment**: Microservice im Container, mit Playwright-Browser-Binaries gebundelt
* **Scaling**:

  * Horizontal ‚Üí mehrere Crawler-Instanzen pro Projekt
  * Vertikal ‚Üí parallele Browser-Sessions pro Instanz
* **Queueing**: Kafka-Topic `crawl.request` / `crawl.result` (bei verteiltem Betrieb)
* **Observability**:

  * Metrics: Pages crawled/sec, Error Rate, Avg Latency
  * Tracing: OpenTelemetry-Spans pro URL
* **Persistence**: CrawlResult im Filesystem oder Memory MCP

---

## 8. Erweiterungspotenzial & offene Fragen

* **Adaptive Crawling**: Priorisierung wichtiger Seiten (hohes Traffic-Potential, viele interne Links)
* **Incremental Crawling**: nur ge√§nderte Seiten neu laden, Delta-Updates speichern
* **Media Focus**: optionale Tiefenanalyse f√ºr Bilder/Videos (Aufl√∂sung, Exif, Lizenzhinweise)
* **Preview Assets**: Screenshots in CrawlResult einbetten (f√ºr direkte Vergleiche)
* **Offene Frage**: Soll der Crawler aggressiv auf Single-Page-Apps eingehen (alle States simulieren), oder nur initialen DOM?

---

üìÑ **Fazit**:
A2 ist ein ressourcenintensiver, aber hochkritischer Microservice. Ohne valide Crawl-Daten sind nachgelagerte Analysen (SEO, A11y, Security) wertlos. Die klare Trennung von **Fetch (leichtgewichtig)** und **Render (schwergewichtig)** ist essenziell, um Skalierung und Effizienz zu gew√§hrleisten.
---

## Aktueller Implementierungsstand

**Bereits funktionsf√§hig**

- Breitensuche √ºber HTML-Seiten mit Link-Normalisierung und Domain-Filter.
- Maximale Seitenausbeute per CLI konfigurierbar.

**Offene Schritte bis zur Production-Readiness**

- JavaScript-Rendering und Asset-Download f√ºr komplexe SPAs.
- Adaptive Crawl-Strategien (Priorisierung, Rate-Limiting, Robots-Compliance).

